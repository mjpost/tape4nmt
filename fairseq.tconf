#import "confs/wmt18-de-en.tconf"
# import "confs/wmt18-en-de.tconf"
# import "confs/wmt18-ru-en.tconf"
# import "confs/wmt18-en-ru.tconf"

global {
  ##################################################################################################
  # General options you should set for your environment
  ##################################################################################################

  # All ducttape files will be written underneath this directory
  ducttape_output="out"

  ##################################################################################################
  # Training options
  ##################################################################################################

  # all default is consistent with nematus
  train_train_from="" # if there is a previous model to start with
  train_train_from_state_dict="" # if there is a previous dict to start with
  train_start_epoch="" # if trained for certain amount of epochs previously

  train_batch_size="80"
  train_optim="adam"
  train_dropout=(Dropout: 0.1 0.3 0.5)
  # Increase for bigger batches (e.g., to 0.001 with --update-freq 16)
  train_lr="0.0005"
  train_lr_min="1e-09"
  train_lr_shrink="0.5"
  train_lr_scheduler="inverse_sqrt"
  train_weight_decay="0.0"
  train_warmup_init_lr="1e-07"
  train_warmup_updates="4000"
  train_criterion="label_smoothed_cross_entropy"
  train_label_smoothing="0.1"
  train_clip_norm=(ClipNorm: 0.0 0.1 0.5 1 5)
  train_max_tokens="4000"
  train_arch=(Architecture: transformer="transformer" conf="fconv" fconv_iwslt_de_en="fconv_iwslt_de_en" transformer_iwslt_de_en="transformer_iwslt_de_en")
    # args.encoder_embed_path = getattr(args, 'encoder_embed_path', None)
    # args.encoder_embed_dim = getattr(args, 'encoder_embed_dim', 512)
    # args.encoder_ffn_embed_dim = getattr(args, 'encoder_ffn_embed_dim', 2048)
    # args.encoder_layers = getattr(args, 'encoder_layers', 6)
    # args.encoder_attention_heads = getattr(args, 'encoder_attention_heads', 8)
    # args.encoder_normalize_before = getattr(args, 'encoder_normalize_before', False)
    # args.encoder_learned_pos = getattr(args, 'encoder_learned_pos', False)
    # args.decoder_embed_path = getattr(args, 'decoder_embed_path', None)
    # args.decoder_embed_dim = getattr(args, 'decoder_embed_dim', args.encoder_embed_dim)
    # args.decoder_ffn_embed_dim = getattr(args, 'decoder_ffn_embed_dim', args.encoder_ffn_embed_dim)
    # args.decoder_layers = getattr(args, 'decoder_layers', 6)
    # args.decoder_attention_heads = getattr(args, 'decoder_attention_heads', 8)
    # args.decoder_normalize_before = getattr(args, 'decoder_normalize_before', False)
    # args.decoder_learned_pos = getattr(args, 'decoder_learned_pos', False)
    # args.attention_dropout = getattr(args, 'attention_dropout', 0.)
    # args.relu_dropout = getattr(args, 'relu_dropout', 0.)
    # args.dropout = getattr(args, 'dropout', 0.1)
    # args.adaptive_softmax_cutoff = getattr(args, 'adaptive_softmax_cutoff', None)
    # args.adaptive_softmax_dropout = getattr(args, 'adaptive_softmax_dropout', 0)
    # args.share_decoder_input_output_embed = getattr(args, 'share_decoder_input_output_embed', False)
    # args.share_all_embeddings = getattr(args, 'share_all_embeddings', False)
    # args.no_token_positional_embeddings = getattr(args, 'no_token_positional_embeddings', False)


  train_share_input_output_embed="--share-all-embeddings"
  train_skip_invalid_size_inputs_valid_test="yes"
  train_adam_beta1="0.9"
  train_adam_beta2="0.98"

  # TEST CONFIGURATIONS
  test_model_selection_strategy="acc"
  test_max_sent_length="300"
  test_beam_size="12"
  test_batch_size="32"

  # EVALUATION
  sacrebleu_args=""

  ##################################################################################################
  # Job parameters
  ##################################################################################################

  # SGE: generic job flags
  resource_flags="-l mem_free=2g,h_rt=12:00:00"

  # SGE: larger job flags
  resource_flags_16g="-l mem_free=16g,h_rt=12:00:00"

  # SGE: larger job flags
  resource_flags_16cpu="-l mem_free=16g,num_proc=16,h_rt=12:00:00"

  # SGE: flags for training a model
  resource_flags_train="-q gpu.q@@2080 -l gpu=1,mem_free=4g,h_rt=168:00:00"

  # SGE: flags for decoding
  resource_flags_decode="-q gpu.q -l gpu=1,mem_free=4g,h_rt=4:00:00"

  # SGE: flags for notifying about job completion (put in your email address!)
  action_flags="-m ae -M post@cs.jhu.edu"
  action_flags_success="-m ae -M post@cs.jhu.edu"

  # The default submitter: shell (run locally) or sge (run on a grid)
  submitter=(TestMode: no="sge" yes="shell")

  # Virtual env location. This should be a file path to the virtual env you want loaded before tasks.
  # This variable supports both conda and Python's virtualenv. For conda, use "conda:ENV" as the value,
  # where "ENV" is the name of the conda environment that should be loaded. For virtualenv, supply
  # the path to the script that should be loaded.
  pyenv="conda:fairseq"

  ##################################################################################################
  # Preprocessing options
  ##################################################################################################

  # protected patterns for tokenizer ("" for default)
  tokenize_protect=""
  tokenizer=cjk

  # sentencepiece options
  subword_joint_model=yes
  subword_coverage_src=0.995
  subword_coverage_trg=0.995
  sentencepiece_vocab_size=10k
  sentencepiece_model_type="unigram"
  sentencepiece_alpha=0.5

  # no of BPE operations
  bpe_operations=32000

  # options for cleaning training data
  train_maxlen=100
  train_clean_ratio=1

  # MASKING
  masking_add_index=(MaskIndex: yes no)
  masking_pattern_files="/home/hltcoe/mpost/code/tape4nmt/patterns.txt"
  masking_dict_files="/dev/null"
  masking_prob=1.0

  use_cpu=(TestMode: no yes)
}

plan test {
  reach sacrebleu via 
    (DoTokenize: yes) * (SubwordMethod: *) * (TestMode: yes)
}

plan transformer {
  reach sacrebleu via
    (SubwordMethod: bpe) * (DoTokenize: yes) * (Casing: actual)

  reach sacrebleu via
    (SubwordMethod: sentencepiece) * (DoTokenize: no)
}

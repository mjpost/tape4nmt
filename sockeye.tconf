# Define the data sets and language pair.
# Choose one of the following files, or simply paste the contents here.

# import "confs/wmt18-de-en.tconf"
# import "confs/wmt18-en-de.tconf"
# import "confs/wmt18-ru-en.tconf"
# import "confs/wmt18-en-ru.tconf"

global {
  ##################################################################################################
  # General options you should set for your environment
  ##################################################################################################

  # All ducttape files will be written underneath this directory
  ducttape_output="out"

  encoder_type="transformer"
  decoder_type="transformer"

  # MODEL SETTINGS
  num_embed=(ModelSize: small="512:512" medium="768:768")
  num_layers=(ModelSize: small="4:4" medium="6:6")
  transformer_model_size=(ModelSize: small=512 medium=768)
  transformer_attention_heads=(ModelSize: small=8 medium=12)
  transformer_feed_forward_num_hidden=(ModelSize: small=1024 medium=2048)
  train_batch_type="word"
  train_batch_size=28000
  train_checkpoint_freq=5000
  train_max_checkpoints_not_improved=10
  initial_learning_rate=(LearningRate: normal=0.0002 slower=0.00002)
  train_num_decode_and_eval=500

  # TEST CONFIGURATIONS
  test_beam_size=12
  test_batch_size=32
  test_max_sent_length=100

  # SOURCE FACTORS
  # the factors to compute
  source_factors="subword case"
  # how to combine factors: "concat" or "sum"
  source_factors_combine="sum"
  # set to same as num_embed if using combine method "sum"
  source_factors_num_embed="512 512"

  # MASKING
  masking_add_index=(MaskIndex: yes no)
  masking_pattern_files="/home/hltcoe/mpost/code/tape4nmt/patterns.txt"
  masking_dict_files="/dev/null"
  masking_prob=1.0
  masking_unmask=(MaskIndex: yes="none" no="attention")  # values "none", "attention", or "align"

  # EVALUATION
  sacrebleu_args="-lc"

  ##################################################################################################
  # Job submission parameters
  ##################################################################################################

  # SGE: flags for notifying about job completion (put in your email address!)
  action_flags="-m a -M post@cs.jhu.edu"
  action_flags_success="-m ae -M post@cs.jhu.edu"

  # SGE: generic job flags
  resource_flags="-l mem_free=2g,h_rt=12:00:00"

  # SGE: larger job flags
  resource_flags_16g="-l mem_free=16g,h_rt=12:00:00"

  # SGE: larger job flags
  resource_flags_16cpu="-l mem_free=16g,num_proc=16,h_rt=12:00:00"

  # SGE: flags for training a model
  num_devices=2
  resource_flags_train="-q gpu.q@@rtx -l gpu=2,mem_free=20g,h_rt=504:00:00"

  # SGE: flags for decoding
  resource_flags_decode="-q gpu.q@@2080 -l gpu=1,mem_free=4g,h_rt=4:00:00"

  # The default submitter: shell (run locally) or sge (run on a grid)
  submitter="sge"

  # Virtual env location. This should be a file path to the virtual env you want loaded before tasks.
  # This variable supports both conda and Python's virtualenv. For conda, use "conda:ENV" as the value,
  # where "ENV" is the name of the conda environment that should be loaded. For virtualenv, supply
  # the path to the script that should be loaded.
  pyenv="conda:sockeye10"

  ##################################################################################################
  # Preprocessing options
  ##################################################################################################

  # protected patterns for tokenizer ("" for default)
  tokenize_protect=""
  tokenizer=(Tokenizer: none default cjk)

  # generic subword options
  subword_joint_model=(SubwordJoint: yes no)
  subword_vocab_size=(SubwordSize: 5000 10000 5000_10000="5000:10000")

  # sentencepiece options
  sentencepiece_model_type="unigram"
  subword_coverage_src=0.995
  subword_coverage_trg=0.995
  sentencepiece_alpha=0.5

  # options for cleaning training data
  train_maxlen=100
  train_clean_ratio=1
  data_maxlines=-1
  seed=44

  use_cpu=no
}

plan transformer {
  reach sacrebleu via
    (SubwordMethod: sentencepiece) * (Tokenizer: none cjk) * (SubwordSize: *) * (SubwordJoint: no) * (Casing: actual) * (ModelSize: small)
  reach sacrebleu via
    (SubwordMethod: sentencepiece) * (Tokenizer: none) * (SubwordSize: 10000) * (SubwordJoint: no) * (LearningRate: *) * (Casing: actual) * (ModelSize: small)
  reach sacrebleu via
    (SubwordMethod: sentencepiece) * (Tokenizer: none) * (SubwordSize: 10000) * (SubwordJoint: no) * (ModelSize: medium) * (Casing: actual) * (ModelSize: small)
}
